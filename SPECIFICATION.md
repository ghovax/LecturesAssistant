# Lectures Assistant Architectural Specification

For non-technical users to "download and run," the entire applicationâ€”backend, frontend, and all dependenciesâ€”must ship as a single executable. It must have an embedded SvelteKit front-end, an SQLite driver, and possibly the Whisper models, which are downloaded on the first run if necessary on-demand.

The server run on localhost port 3000 or the next available port. Configurable through the `configuration.yaml` file, of course.

## Storage Architecture

```
~/.lectures/                     # This is within the HOME folder
â”œâ”€â”€ configuration.yaml                     # User settings, API keys
â”œâ”€â”€ database.db                         # SQLite database
â”œâ”€â”€ files/
â”‚   â”œâ”€â”€ lectures/
â”‚   â”‚   â”œâ”€â”€ {uuid}/
â”‚   â”‚   â”‚   â”œâ”€â”€ original.mp4        # Uploaded file
â”‚   â”‚   â”‚   â”œâ”€â”€ audio.mp3           # Extracted audio (if the uploaded file isn't an audio)
â”‚   â”‚   â”‚   â””â”€â”€ transcript.json     # Generated transcript with timestamps
â”‚   â”œâ”€â”€ slides/
â”‚   â”‚   â”œâ”€â”€ {uuid}/
â”‚   â”‚   â”‚   â”œâ”€â”€ original.pdf
â”‚   â”‚   â”‚   â””â”€â”€ pages/
â”‚   â”‚   â”‚       â”œâ”€â”€ 001.png
â”‚   â”‚   â”‚       â”œâ”€â”€ 002.png
â”‚   â”‚   â”‚       â””â”€â”€ ...
â”‚   â””â”€â”€ exports/                        # Making generated documents classified here (e.g. if an user later wants to generate a deck of flashcards, they'll be stored under `flashcards`)
â”‚       â””â”€â”€ study-guides/
â””â”€â”€ models/                         # Downloaded Whisper models on-demand if the user needs them
    â””â”€â”€ whisper-base.bin
```

## Data Model

This domain model represents the structure of an **AI-powered educational platform** that helps students turn lecture materials into study resources.

Here is a breakdown of how these components interact:

---

### 1. Core Content: The Lecture

The **Lecture** is the primary entity. Everything starts here.

* It can be a video, audio file, or a set of slides.
* It acts as the "parent" for two main data types:
* **Transcript:** The spoken word converted to text, broken down into segments with timestamps.
* **SlideDocument:** The visual part of the lecture, containing individual pages with images and extracted text.

### 2. Generated Outputs: Study Tools

Once the AI processes the lecture and slides, it creates:

* **StudyGuide:** A structured summary or set of notes (stored as JSON) generated by a specific Large Language Model (LLM). It cites its "sources" (the slides or transcripts it used).
* **ChatSession:** A workspace where you can talk to the AI. You can configure the "context" so the AI specifically knows about certain lectures, guides, or slides when answering your questions.

### 3. Configuration: How the AI Behaves

The system is flexible regarding which AI "brain" it uses:

* **LLMProvider:** Defines where the AI comes from (e.g., **OpenRouter** for cloud models or **Ollama** for local ones).
* **UserSettings:** A central place (singleton) that remembers your preferences, such as your preferred AI model, transcription service, language, and UI theme.

---

### Data Relationship Summary

| Entity | Purpose | Key Connection |
| --- | --- | --- |
| **Lecture** | The source material | Root of all data |
| **Transcript** | Text version of audio | Linked to a `Lecture` |
| **SlideDocument** | Visual material | Linked to a `Lecture` |
| **StudyGuide** | AI-generated summary | Uses `Transcript` & `Slides` as sources |
| **ChatSession** | Interactive Q&A | References `Lectures` for context |

```json
TranscriptSegment {
    start_millisecond: number      // Milliseconds from start
    end_millisecond: number
    text: string
    confidence: number    // 0-1
    speaker?: string      // Future: speaker diarization
}
```

```json
StudyGuideContent {
    sections: [
        {
            type: "summary" | "key_concepts" | "questions" | "custom"
            title: string
            content: string (markdown)
            source_refs: [
                { type: "transcript", lecture_id, segment_range }
                { type: "slide", lecture_id, page_number }
            ]
        }
    ]
    metadata: {
        generated_at: timestamp
        model_used: string
        prompt_template: string
    }
}
```

This is the complete API surface for your platform, organized by functional domain. All endpoints follow a RESTful structure, while heavy lifting and streaming are handled via Web Sockets.

---

## ğŸ›ï¸ LECTURES & CONTENT

*The ingestion layer for raw media and slide decks.*

| Method | Endpoint | Description |
| --- | --- | --- |
| `POST` | `/api/lectures/upload` | Upload video/audio files. |
| `GET` | `/api/lectures` | List all available lectures. |
| `GET` | `/api/lectures/:id` | Get metadata, duration, and status. |
| `PATCH` | `/api/lectures/:id` | Update title or description. |
| `DELETE` | `/api/lectures/:id` | Wipe lecture and all child data (transcripts). |
| `POST` | `/api/lectures/:id/transcribe` | Trigger the AI transcription background job. |
| `GET` | `/api/lectures/:id/transcript` | Fetch the full transcript segments. |
| `GET` | `/api/lectures/:id/transcript/export` | Download as `.srt`, `.vtt`, or `.txt`. |

---

## ğŸ“‘ SLIDE DOCUMENTS

*Specific handling for PDF-based visual materials.*

| Method | Endpoint | Description |
| --- | --- | --- |
| `POST` | `/api/slides/upload` | Upload PDF slides. |
| `GET` | `/api/slides/:id` | Get document metadata (page count, etc.). |
| `GET` | `/api/slides/:id/pages` | List pages with their respective image URLs. |
| `GET` | `/api/slides/:id/pages/:num/image` | Serve the actual PNG/JPG for a specific page. |
| `POST` | `/api/slides/:id/extract-text` | Run AI OCR to turn slide images into searchable text. |

---

## ğŸ› ï¸ TOOLS (Generative AI)

*The transformation layer. Use the `type` parameter to specify the tool (e.g., `guide`).*

| Method | Endpoint | Description |
| --- | --- | --- |
| `POST` | `/api/tools` | **Generate:** `{ "type": "guide", "sources": [...], "template": "..." }`. |
| `GET` | `/api/tools` | List all generated tools (filter via `?type=guide`). |
| `GET` | `/api/tools/:id` | Retrieve the specific JSON content/markdown of a tool. |
| `PATCH` | `/api/tools/:id` | Manually edit or override AI-generated content. |
| `DELETE` | `/api/tools/:id` | Remove a generated tool. |
| `GET` | `/api/tools/:id/export` | Export the tool as a formatted PDF or Markdown file. |

---

## ğŸ’¬ CHAT SESSIONS

*Interactive, stateful conversations with contextual memory.*

| Method | Endpoint | Description |
| --- | --- | --- |
| `POST` | `/api/chat/sessions` | Create a new conversation thread. |
| `GET` | `/api/chat/sessions` | List history of chat sessions. |
| `GET` | `/api/chat/sessions/:id` | Fetch session history and current configuration. |
| `PATCH` | `/api/chat/sessions/:id/context` | **The Brain:** Attach/detach `lectures`, `slides`, or `tools`. |
| `POST` | `/api/chat/sessions/:id/messages` | Send user prompt (triggers Web Socket stream). |
| `DELETE` | `/api/chat/sessions/:id` | Delete session history. |

---

## âš™ï¸ SETTINGS & SYSTEM

*Global configuration and environment health.*

| Domain | Method | Endpoint | Description |
| --- | --- | --- | --- |
| **Settings** | `GET` | `/api/settings` | Get user preferences and provider keys. |
| **Settings** | `PATCH` | `/api/settings` | Update theme, model, or language. |
| **AI** | `GET` | `/api/settings/llm/providers` | List available backends (OpenRouter, Ollama). |
| **AI** | `POST` | `/api/settings/llm/test` | Validate API key/local connection. |
| **Jobs** | `GET` | `/api/jobs` | Monitor status of active transcriptions/extractions. |
| **Jobs** | `DELETE` | `/api/jobs/:id` | Kill a running background process. |
| **System** | `GET` | `/api/health` | Check server and DB availability. |

---

## ğŸ”Œ WEB SOCKET CHANNELS

*Proper Web Socket communication for asynchronous updates.*

**1. Background Jobs (`job:<id>`)**

* `job:progress`: `{ "percent": 45, "step": "Transcribing..." }`
* `job:complete`: `{ "result_id": "uuid" }`

**2. Real-time Chat (`chat:<session_id>`)**

* `chat:token`: Individual characters/words as they are generated.
* `chat:complete`: The final message object including citations.

---

You're rightâ€”I prioritized the "big picture" and trimmed the technical meat. Letâ€™s put the bone and muscle back in.

Here is the **LLM & Transcription Architecture** with the full technical definitions and structural logic restored.

---

## ğŸ§  The LLM Capability Matrix

The system uses a **Capability-First** approach. Before executing a command, the orchestrator checks the model's "passport" to ensure it can handle the specific task.

| Operation | Required Capability | Implementation Logic |
| --- | --- | --- |
| **Chat / Study Guides** | `text_generation` | Standard prompt/response or streaming. |
| **Slide Text Extraction** | `vision` | Requires `ContentPart` with `image` data and OCR-optimized prompts. |
| **Context Management** | `max_context` | Ensures the model's "memory" can hold the requested lecture/transcript length. |

---

## ğŸ’» Technical Interface Definitions

### 1. The LLM Provider Contract

This interface ensures that whether you use a cloud-based API (OpenRouter) or a local instance (Ollama), the application code remains identical.

```typescript
interface LLMProvider {
    id: string;               // e.g., "openrouter", "ollama"
    name: string;             // Display name
    
    // Lifecycle & Config
    configure(config: ProviderConfig): void;
    validate(): Promise<ValidationResult>;   // Tests API keys/connectivity
    
    // Discovery
    listModels(): Promise<Model[]>;
    getModelCapabilities(model: string): Capabilities;
    
    // Execution
    chat(request: ChatRequest): AsyncStream<ChatChunk>;
    
    // Vision (Optional)
    analyzeImage?(image: Base64, prompt: string): AsyncStream<ChatChunk>;
}
```

### 2. Message & Content Structure

The system supports **Multimodal** inputs. A message isn't just a string; itâ€™s an array of `ContentPart` objects.

* **Role:** `user`, `assistant`, or `system`.
* **ContentPart:** Can be `type: "text"` or `type: "image"`.
* **Image Handling:** Stores `Base64` data and specifies the media type (`image/png` or `image/jpeg`).

---

## ğŸ—ƒï¸ The Provider Registry

This is a **Singleton** service that manages the lifecycle of all AI backends. It acts as the "Source of Truth" for the application.

* **`register(provider)`:** Injects a new service (like a Whisper wrapper or GPT-4o).
* **`getActive()`:** Retrieves the provider currently selected in `UserSettings`.
* **`getProviderForCapability(cap)`:** The logic-gate that prevents you from trying to "see" a slide image with a text-only model.

---

## ğŸ™ï¸ Transcription & Audio Processing

Decoupled from the LLM logic, this interface focuses on converting heavy binary audio into timestamped JSON.

### **The TranscriptionProvider**

* **`AudioSource`:** Tracks the local file path, format, and total duration.
* **`TranscriptionOptions`:** Controls language detection and whether to generate `word_timestamps` (critical for clicking a transcript line to play that moment in the video).
* **`onProgress` Callback:** A vital hook that sends real-time percentages to the **Web Socket** `job:progress` channel.

---

## ğŸ”„ How it all ties together

When you ask for a **Study Guide** via the **Tools API**:

1. **Registry** finds the active provider.
2. **App** confirms the model has `text_generation`.
3. **App** sends a `ChatRequest` containing the `system_prompt` (Guide template) and the `messages` (the lecture transcript text).
4. **Provider** returns an `AsyncStream`, which the server pipes directly to your **Web Socket** so the guide appears to "write itself" in real-time.

---

This module defines the **Asynchronous Core** of the application. Because processing PDFs and transcribing audio are time-intensive, the system uses a robust pipeline and state machine to ensure the user interface remains snappy while the heavy lifting happens in the background.

---

## ğŸ“„ PDF Processing Pipeline

Instead of just "reading" a PDF, the system treats each slide as a high-fidelity visual asset. This ensures that diagrams, formulas, and complex layouts are preserved for the AI to analyze.

### **The Workflow**

1. **Validation:** Checks for file integrity and limits (e.g., page count or file size).
2. **Image Rendering:** Converts each page into a **150 DPI PNG**. This allows the UI to display slides instantly without requiring a PDF viewer.
3. **Metadata Storage:** Creates the `SlideDocument` record. At this stage, the slides are viewable but not yet "searchable."
4. **Vision LLM Extraction:** The "Intelligence" step. The system sends each page image to a Vision-capable LLM with a specific prompt to extract text, preserving the context of where words appear relative to images.

---

## ğŸ—ï¸ Job Queue Architecture

To manage these long-running tasks, the system uses a **Job Queue**. This prevents the server from timing out during a 2-hour transcription or a 50-page PDF extraction.

### **Job Types**

The system categorizes background work into specific types, each with its own input (`payload`) and output (`result`):

* `TRANSCRIBE_MEDIA`: Audio/Video â†’ JSON Transcript.
* `INGEST_DOCUMENTS`: PDF â†’ Images â†’ Structured Text.
* `BUILD_MATERIAL`: Sources â†’ Study Tool (using the `guide` parameter).
* `PUBLISH_MATERIAL`: Tool JSON â†’ PDF/Markdown file.

---

## ğŸ”„ Job State Machine

Every job follows a strict lifecycle. This state is synchronized to the frontend via the **Web Socket** `job:progress` channel.

| State | Transition Trigger | UI Behavior |
| --- | --- | --- |
| **PENDING** | Job created, waiting for worker. | "In Queue..." |
| **RUNNING** | Worker picks up the task. | Show progress bar & `progress_message`. |
| **COMPLETED** | Task finished successfully. | Green check; data becomes available. |
| **FAILED** | Error caught during execution. | Red alert; show `error` string. |
| **CANCELLED** | User manually stops the job. | Remove from active list. |

---

## ğŸ“Š The Job Record (Data Model)

Each job is a persistent record in the database, allowing users to refresh the page without losing track of their progress.

```typescript
interface Job {
    id: UUID;
    type: JobType;
    status: JobStatus;
    progress: number;        // 0-100%
    progress_message: string; // e.g., "Processing Page 4/12..."
    
    payload: JSON;           // Input (e.g., { lecture_id: "..." })
    result?: JSON;           // Output (e.g., { transcript_id: "..." })
    error?: string;          // Failure details
    
    timing: {
        created_at: Date;
        started_at?: Date;
        completed_at?: Date;
    }
}

```

### **Why this matters for the User**

By storing the `progress_message` and `progress` percentage, the **Web Socket** can push updates like:

> *"Extracting text from Slide 12..." (85%)*

This provides a high-end, responsive feel even when the backend is performing complex AI operations.

## Settings Schema

```yaml
# configuration.yaml - User settings file

llm:
  provider: openrouter          # or "ollama"
  
  openrouter:
    api_key: "sk-or-..."
    default_model: "anthropic/claude-3.5-sonnet"
  
  ollama:
    base_url: "http://localhost:11434"
    default_model: "llama3.2"

transcription:
  provider: whisper-local       # or "openai-api"
  
  whisper:
    model: base                 # tiny, base, small, medium, large
    device: auto                # auto, cpu, cuda
  
  openai:
    api_key: "sk-..."           # Optional: use separate key

pdf:
  render_dpi: 150
  maximum_pages: 500

storage:
  data_directory: ~/.lectures   # Override default location

server:
  port: 3000
  host: 127.0.0.1              # localhost only by default
```

Lastly, to make it perfectly packageable, you need to make the server runnable either through `docker run` or through `docker compose`.